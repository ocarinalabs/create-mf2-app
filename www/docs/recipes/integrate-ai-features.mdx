---
title: "Integrate AI Features"
description: "Add GPT-powered features to your MF2 app"
icon: "robot"
---

# AI-Powered Features

Stop calling them "AI wrappers." You're building real products that happen to use AI. Here's how to integrate OpenAI, Claude, or any LLM into your MF2 app the right way.

<Note>
  **Cost warning**: AI features can get expensive fast. We'll show you how to control costs while shipping amazing features.
</Note>

## What you'll build

- ðŸ¤– Smart text generation
- ðŸ“ AI-powered forms
- ðŸ” Semantic search
- ðŸ’¬ Chat interfaces
- ðŸŽ¨ Image generation
- ðŸ“Š Usage tracking & limits

## The smart setup (5 min)

### 1. Install dependencies

```bash
npm install openai ai zod
```

### 2. Add your API keys

```env .env.local
# Choose your fighter
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...
```

### 3. Create the AI service

```typescript convex/ai.ts
import { action } from "./_generated/server";
import { v } from "convex/values";
import OpenAI from "openai";
import { z } from "zod";

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

// Track usage for billing
export const complete = action({
  args: {
    prompt: v.string(),
    model: v.optional(v.string()),
    maxTokens: v.optional(v.number()),
  },
  handler: async (ctx, args) => {
    // Get user and check limits
    const user = await ctx.runQuery(api.users.getCurrentUser);
    if (!user) throw new Error("Not authenticated");
    
    // Check usage limits
    const usage = await ctx.runQuery(api.ai.getMonthlyUsage);
    if (usage.tokensUsed > usage.tokenLimit) {
      throw new Error("Monthly AI limit reached. Upgrade for more.");
    }
    
    // Make the call
    const completion = await openai.chat.completions.create({
      model: args.model || "gpt-3.5-turbo",
      messages: [{ role: "user", content: args.prompt }],
      max_tokens: args.maxTokens || 500,
      temperature: 0.7,
    });
    
    // Track usage
    await ctx.runMutation(api.ai.trackUsage, {
      userId: user._id,
      tokensUsed: completion.usage?.total_tokens || 0,
      cost: calculateCost(completion.usage),
      model: args.model || "gpt-3.5-turbo",
    });
    
    return completion.choices[0].message.content;
  },
});

function calculateCost(usage: any): number {
  // GPT-3.5: $0.0015 per 1K input, $0.002 per 1K output
  const inputCost = (usage.prompt_tokens / 1000) * 0.0015;
  const outputCost = (usage.completion_tokens / 1000) * 0.002;
  return inputCost + outputCost;
}
```

## Feature 1: Smart content generation (10 min)

Let users generate content with templates:

```tsx src/components/ai-writer.tsx
"use client";

import { useState } from "react";
import { useAction } from "convex/react";
import { api } from "@/convex/_generated/api";
import { Button } from "@/components/ui/button";
import { Textarea } from "@/components/ui/textarea";
import {
  Select,
  SelectContent,
  SelectItem,
  SelectTrigger,
  SelectValue,
} from "@/components/ui/select";
import { Card } from "@/components/ui/card";
import { Loader2, Sparkles } from "lucide-react";

const TEMPLATES = {
  blog: {
    name: "Blog Post",
    prompt: "Write a blog post about {topic}. Make it engaging, informative, and around {length} words. Include a catchy title and clear sections.",
    fields: [
      { name: "topic", label: "Topic", type: "text" },
      { name: "length", label: "Length", type: "select", options: ["500", "1000", "1500"] },
    ],
  },
  email: {
    name: "Email",
    prompt: "Write a professional email to {recipient} about {subject}. The tone should be {tone}. Keep it concise and actionable.",
    fields: [
      { name: "recipient", label: "Recipient", type: "text" },
      { name: "subject", label: "Subject", type: "text" },
      { name: "tone", label: "Tone", type: "select", options: ["formal", "friendly", "casual"] },
    ],
  },
  product: {
    name: "Product Description",
    prompt: "Write a compelling product description for {product}. Highlight: {features}. Target audience: {audience}. Include benefits and a call to action.",
    fields: [
      { name: "product", label: "Product Name", type: "text" },
      { name: "features", label: "Key Features", type: "textarea" },
      { name: "audience", label: "Target Audience", type: "text" },
    ],
  },
};

export function AIWriter() {
  const [template, setTemplate] = useState<keyof typeof TEMPLATES>("blog");
  const [values, setValues] = useState<Record<string, string>>({});
  const [output, setOutput] = useState("");
  const [loading, setLoading] = useState(false);
  const generate = useAction(api.ai.complete);

  const handleGenerate = async () => {
    setLoading(true);
    try {
      // Fill in template
      let prompt = TEMPLATES[template].prompt;
      Object.entries(values).forEach(([key, value]) => {
        prompt = prompt.replace(`{${key}}`, value);
      });
      
      const result = await generate({ prompt });
      setOutput(result);
    } catch (error) {
      console.error(error);
      // Handle error (show toast, etc)
    } finally {
      setLoading(false);
    }
  };

  return (
    <div className="space-y-6">
      <Card className="p-6">
        <div className="space-y-4">
          <div>
            <label className="text-sm font-medium">Template</label>
            <Select
              value={template}
              onValueChange={(value) => {
                setTemplate(value as keyof typeof TEMPLATES);
                setValues({});
              }}
            >
              <SelectTrigger>
                <SelectValue />
              </SelectTrigger>
              <SelectContent>
                {Object.entries(TEMPLATES).map(([key, tpl]) => (
                  <SelectItem key={key} value={key}>
                    {tpl.name}
                  </SelectItem>
                ))}
              </SelectContent>
            </Select>
          </div>

          {TEMPLATES[template].fields.map((field) => (
            <div key={field.name}>
              <label className="text-sm font-medium">{field.label}</label>
              {field.type === "text" && (
                <Input
                  value={values[field.name] || ""}
                  onChange={(e) =>
                    setValues({ ...values, [field.name]: e.target.value })
                  }
                />
              )}
              {field.type === "textarea" && (
                <Textarea
                  value={values[field.name] || ""}
                  onChange={(e) =>
                    setValues({ ...values, [field.name]: e.target.value })
                  }
                />
              )}
              {field.type === "select" && (
                <Select
                  value={values[field.name] || ""}
                  onValueChange={(value) =>
                    setValues({ ...values, [field.name]: value })
                  }
                >
                  <SelectTrigger>
                    <SelectValue />
                  </SelectTrigger>
                  <SelectContent>
                    {field.options?.map((option) => (
                      <SelectItem key={option} value={option}>
                        {option}
                      </SelectItem>
                    ))}
                  </SelectContent>
                </Select>
              )}
            </div>
          ))}

          <Button
            onClick={handleGenerate}
            disabled={loading || Object.keys(values).length === 0}
            className="w-full"
          >
            {loading ? (
              <Loader2 className="mr-2 h-4 w-4 animate-spin" />
            ) : (
              <Sparkles className="mr-2 h-4 w-4" />
            )}
            Generate Content
          </Button>
        </div>
      </Card>

      {output && (
        <Card className="p-6">
          <div className="space-y-4">
            <div className="flex justify-between items-center">
              <h3 className="font-semibold">Generated Content</h3>
              <Button
                variant="outline"
                size="sm"
                onClick={() => navigator.clipboard.writeText(output)}
              >
                Copy
              </Button>
            </div>
            <div className="prose max-w-none">
              <pre className="whitespace-pre-wrap">{output}</pre>
            </div>
          </div>
        </Card>
      )}
    </div>
  );
}
```

## Feature 2: AI-powered search (10 min)

Add semantic search using embeddings:

```typescript convex/search.ts
import { action, mutation, query } from "./_generated/server";
import { v } from "convex/values";
import OpenAI from "openai";

const openai = new OpenAI();

// Generate embedding for text
export const generateEmbedding = action({
  args: { text: v.string() },
  handler: async (ctx, args) => {
    const response = await openai.embeddings.create({
      model: "text-embedding-3-small",
      input: args.text,
    });
    
    return response.data[0].embedding;
  },
});

// Store content with embeddings
export const indexContent = mutation({
  args: {
    content: v.string(),
    metadata: v.any(),
  },
  handler: async (ctx, args) => {
    // Generate embedding
    const embedding = await ctx.scheduler.runAfter(
      0,
      api.search.generateEmbedding,
      { text: args.content }
    );
    
    // Store in database
    await ctx.db.insert("searchIndex", {
      content: args.content,
      embedding,
      metadata: args.metadata,
      createdAt: Date.now(),
    });
  },
});

// Semantic search
export const search = action({
  args: {
    query: v.string(),
    limit: v.optional(v.number()),
  },
  handler: async (ctx, args) => {
    // Generate query embedding
    const queryEmbedding = await openai.embeddings.create({
      model: "text-embedding-3-small",
      input: args.query,
    });
    
    // Get all indexed content
    const allContent = await ctx.runQuery(api.search.getAllIndexed);
    
    // Calculate cosine similarity
    const results = allContent
      .map((item) => ({
        ...item,
        similarity: cosineSimilarity(
          queryEmbedding.data[0].embedding,
          item.embedding
        ),
      }))
      .sort((a, b) => b.similarity - a.similarity)
      .slice(0, args.limit || 10);
    
    return results;
  },
});

function cosineSimilarity(a: number[], b: number[]): number {
  const dotProduct = a.reduce((sum, val, i) => sum + val * b[i], 0);
  const magnitudeA = Math.sqrt(a.reduce((sum, val) => sum + val * val, 0));
  const magnitudeB = Math.sqrt(b.reduce((sum, val) => sum + val * val, 0));
  return dotProduct / (magnitudeA * magnitudeB);
}
```

Search UI:

```tsx src/components/ai-search.tsx
"use client";

import { useState } from "react";
import { useAction } from "convex/react";
import { api } from "@/convex/_generated/api";
import { Input } from "@/components/ui/input";
import { Search } from "lucide-react";

export function AISearch() {
  const [query, setQuery] = useState("");
  const [results, setResults] = useState([]);
  const search = useAction(api.search.search);

  const handleSearch = async (e: React.FormEvent) => {
    e.preventDefault();
    if (!query.trim()) return;
    
    const searchResults = await search({ query });
    setResults(searchResults);
  };

  return (
    <div className="space-y-4">
      <form onSubmit={handleSearch} className="relative">
        <Search className="absolute left-3 top-3 h-4 w-4 text-muted-foreground" />
        <Input
          value={query}
          onChange={(e) => setQuery(e.target.value)}
          placeholder="Search using natural language..."
          className="pl-9"
        />
      </form>
      
      <div className="space-y-2">
        {results.map((result, i) => (
          <SearchResult key={i} result={result} />
        ))}
      </div>
    </div>
  );
}
```

## Feature 3: Chat interface (15 min)

Build a proper chat with streaming:

```tsx src/app/chat/page.tsx
"use client";

import { useChat } from "ai/react";
import { Button } from "@/components/ui/button";
import { Input } from "@/components/ui/input";
import { ScrollArea } from "@/components/ui/scroll-area";
import { Avatar } from "@/components/ui/avatar";
import { Send } from "lucide-react";

export default function ChatPage() {
  const { messages, input, handleInputChange, handleSubmit, isLoading } = useChat({
    api: "/api/chat",
  });

  return (
    <div className="flex flex-col h-[600px] max-w-2xl mx-auto">
      <ScrollArea className="flex-1 p-4">
        <div className="space-y-4">
          {messages.map((message) => (
            <ChatMessage key={message.id} message={message} />
          ))}
          {isLoading && (
            <div className="flex items-center space-x-2">
              <div className="animate-pulse h-2 w-2 bg-gray-500 rounded-full" />
              <div className="animate-pulse h-2 w-2 bg-gray-500 rounded-full animation-delay-200" />
              <div className="animate-pulse h-2 w-2 bg-gray-500 rounded-full animation-delay-400" />
            </div>
          )}
        </div>
      </ScrollArea>
      
      <form onSubmit={handleSubmit} className="p-4 border-t">
        <div className="flex gap-2">
          <Input
            value={input}
            onChange={handleInputChange}
            placeholder="Ask anything..."
            disabled={isLoading}
          />
          <Button type="submit" disabled={isLoading || !input.trim()}>
            <Send className="h-4 w-4" />
          </Button>
        </div>
      </form>
    </div>
  );
}

function ChatMessage({ message }: { message: any }) {
  const isUser = message.role === "user";
  
  return (
    <div className={`flex gap-3 ${isUser ? "justify-end" : ""}`}>
      {!isUser && (
        <Avatar className="h-8 w-8">
          <div className="bg-gradient-to-br from-blue-500 to-purple-500 h-full w-full rounded-full" />
        </Avatar>
      )}
      <div
        className={`rounded-lg px-4 py-2 max-w-[80%] ${
          isUser
            ? "bg-primary text-primary-foreground"
            : "bg-muted"
        }`}
      >
        <p className="text-sm">{message.content}</p>
      </div>
      {isUser && (
        <Avatar className="h-8 w-8">
          <div className="bg-gradient-to-br from-green-500 to-blue-500 h-full w-full rounded-full" />
        </Avatar>
      )}
    </div>
  );
}
```

API route with streaming:

```typescript src/app/api/chat/route.ts
import { StreamingTextResponse, LangChainStream } from "ai";
import { ChatOpenAI } from "langchain/chat_models/openai";
import { auth } from "@clerk/nextjs";

export async function POST(req: Request) {
  const { userId } = auth();
  if (!userId) {
    return new Response("Unauthorized", { status: 401 });
  }

  const { messages } = await req.json();
  const { stream, handlers } = LangChainStream();

  const llm = new ChatOpenAI({
    modelName: "gpt-3.5-turbo",
    streaming: true,
    callbacks: [handlers],
  });

  llm
    .call(messages)
    .catch((e) => console.error("LLM Error:", e));

  return new StreamingTextResponse(stream);
}
```

## Cost control & limits (5 min)

Track and limit usage per user:

```typescript convex/schema.ts
// Add to your schema
aiUsage: defineTable({
  userId: v.id("users"),
  workspaceId: v.id("workspaces"),
  month: v.string(), // "2024-01"
  tokensUsed: v.number(),
  apiCalls: v.number(),
  totalCost: v.number(),
})
  .index("by_user_month", ["userId", "month"])
  .index("by_workspace_month", ["workspaceId", "month"]),
```

Usage tracking:

```typescript convex/ai.ts
export const trackUsage = mutation({
  args: {
    userId: v.id("users"),
    tokensUsed: v.number(),
    cost: v.number(),
    model: v.string(),
  },
  handler: async (ctx, args) => {
    const month = new Date().toISOString().slice(0, 7);
    
    const existing = await ctx.db
      .query("aiUsage")
      .withIndex("by_user_month", q => 
        q.eq("userId", args.userId).eq("month", month)
      )
      .first();
    
    if (existing) {
      await ctx.db.patch(existing._id, {
        tokensUsed: existing.tokensUsed + args.tokensUsed,
        apiCalls: existing.apiCalls + 1,
        totalCost: existing.totalCost + args.cost,
      });
    } else {
      await ctx.db.insert("aiUsage", {
        userId: args.userId,
        month,
        tokensUsed: args.tokensUsed,
        apiCalls: 1,
        totalCost: args.cost,
      });
    }
  },
});

// Check limits before AI calls
export const checkUsageLimits = query({
  handler: async (ctx) => {
    const user = await getCurrentUser(ctx);
    if (!user) return { allowed: false };
    
    const month = new Date().toISOString().slice(0, 7);
    const usage = await ctx.db
      .query("aiUsage")
      .withIndex("by_user_month", q => 
        q.eq("userId", user._id).eq("month", month)
      )
      .first();
    
    // Free tier: 1000 tokens/month
    // Pro: 50,000 tokens/month
    const limits = {
      free: 1000,
      pro: 50000,
      enterprise: 1000000,
    };
    
    const limit = limits[user.plan || "free"];
    const used = usage?.tokensUsed || 0;
    
    return {
      allowed: used < limit,
      used,
      limit,
      remaining: limit - used,
    };
  },
});
```

## Pro tips

### 1. Cache AI responses

```typescript
// Check cache first
const cached = await ctx.db
  .query("aiCache")
  .withIndex("by_prompt_hash", q => 
    q.eq("promptHash", hashPrompt(prompt))
  )
  .first();

if (cached && cached.createdAt > Date.now() - 24 * 60 * 60 * 1000) {
  return cached.response;
}
```

### 2. Rate limit by IP

```typescript
const ip = request.headers.get("x-forwarded-for");
const recentRequests = await getRecentRequests(ip);

if (recentRequests > 10) {
  return new Response("Rate limit exceeded", { status: 429 });
}
```

### 3. Stream large responses

```typescript
// Use streaming for better UX
const stream = await openai.chat.completions.create({
  model: "gpt-4",
  messages,
  stream: true,
});

for await (const chunk of stream) {
  // Send chunk to client
}
```

## Common AI features to add

<AccordionGroup>
  <Accordion title="Smart notifications" icon="bell">
    ```typescript
    // Summarize activity for users
    const summary = await generateSummary({
      activities: userActivities,
      style: "concise",
    });
    ```
  </Accordion>
  
  <Accordion title="Auto-tagging" icon="tag">
    ```typescript
    // Automatically categorize content
    const tags = await generateTags({
      content: article.text,
      existingTags: allTags,
    });
    ```
  </Accordion>
  
  <Accordion title="Content moderation" icon="shield">
    ```typescript
    // Check user content
    const moderation = await checkContent({
      text: comment.body,
      rules: communityGuidelines,
    });
    ```
  </Accordion>
</AccordionGroup>

## Deployment checklist

- [ ] Set conservative rate limits
- [ ] Add cost alerts ($50, $100, $500)
- [ ] Cache common requests
- [ ] Log all AI interactions
- [ ] Set up fallbacks for API failures
- [ ] Test with free tier limits

## You're now AI-powered! ðŸ¤–

Your app can now:
- Generate content on demand
- Search semantically
- Chat with users
- Control costs automatically
- Scale with usage

Next steps:

<CardGroup cols={2}>
  <Card title="Fine-tune models" icon="sliders" href="/recipes/fine-tuning">
    Train on your data
  </Card>
  <Card title="Voice features" icon="microphone" href="/recipes/voice-ai">
    Add speech-to-text
  </Card>
</CardGroup>

Remember: AI is a feature, not the product. Use it to make your users' lives easier, not to show off GPT-4.